{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro_to_tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morbosohex/Workflow/blob/master/intro_to_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx4ztKHcqAcL",
        "colab_type": "text"
      },
      "source": [
        "# Pre-lab: exerices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6ZXAF6FqQor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6mmaS_RqP5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hello_constant = tf.constant('hello world!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfMdCTq0qPuy",
        "colab_type": "code",
        "outputId": "b2790530-b1f4-488b-c7dc-8cb8ca2ede7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    output = sess.run(hello_constant)\n",
        "    print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'hello world!'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85BP9plqqPWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.Variable(5)\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session as sess:\n",
        "    sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCcnllbLqOfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_features = 120\n",
        "n_labels = 5\n",
        "\n",
        "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
        "bias = tf.Variable(tf.zeros(n_labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNU4XPD2_Vkp",
        "colab_type": "text"
      },
      "source": [
        "# Quiz - train weights using the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rs9sWMZ_t3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "def get_weights(n_features, n_labels):\n",
        "    \"\"\"\n",
        "    Return TensroFlow weights\n",
        "    :param n_features: Number of features\n",
        "    :param n_labels: Number of labels\n",
        "    :return: TensorFlow weights\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: Return weights\n",
        "    return tf.Variable(tf.truncated_normal((n_features,n_labels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBatk_5Z_uX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_biases(n_labels):\n",
        "    \"\"\"\n",
        "    Return TensorFlow bias\n",
        "    :param n_labels: Number of labels\n",
        "    :return: TensorFlow bias\n",
        "    \"\"\"\n",
        "    # TODO: Return biases\n",
        "    return tf.Variable(tf.zeros(n_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD-Hy2HM_uei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jdef linear(input, w, b):\n",
        "    \"\"\"\n",
        "    Return linear function in TensorFlow\n",
        "    :param input: TensorFlow input\n",
        "    :param w: TensorFlow weights\n",
        "    :param b: TensorFlow biases\n",
        "    :return: TensorFlow linear function\n",
        "    \"\"\"\n",
        "    # TODO: Linear Function (xW + b)\n",
        "    return tf.add(tf.matmul(input, w), b)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhAVPwSO_uki",
        "colab_type": "code",
        "outputId": "56598e3c-679f-48ba-d8ad-45fa0549658a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "def mnist_features_labels(n_labels):\n",
        "    \"\"\"\n",
        "    Gets the first <n> labels from the MNIST dataset\n",
        "    :param n_labels: Number of labels to use\n",
        "    :return: Tuple of feature list and label list\n",
        "    \"\"\"\n",
        "    mnist_features = []\n",
        "    mnist_labels = []\n",
        "\n",
        "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
        "\n",
        "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
        "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
        "\n",
        "        # Add features and labels if it's for the first <n>th labels\n",
        "        if mnist_label[:n_labels].any():\n",
        "            mnist_features.append(mnist_feature)\n",
        "            mnist_labels.append(mnist_label[:n_labels])\n",
        "\n",
        "    return mnist_features, mnist_labels\n",
        "\n",
        "# Number of features (28*28 image is 784 features)\n",
        "n_features = 784\n",
        "# Number of labels\n",
        "n_labels = 3\n",
        "\n",
        "features = tf.placeholder(tf.float32)\n",
        "labels = tf.placeholder(tf.float32)\n",
        "\n",
        "# Weights and Biases\n",
        "w = get_weights(n_features, n_labels)\n",
        "b = get_biases(n_labels)\n",
        "\n",
        "# Linear Function xW + b\n",
        "logits = linear(features, w, b)\n",
        "\n",
        "# Training data\n",
        "train_features, train_labels = mnist_features_labels(n_labels)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V6u43BuFuih",
        "colab_type": "code",
        "outputId": "81828b68-c8a4-45b0-e460-89f4803a095a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session() as session:\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Softmax\n",
        "    prediction = tf.nn.softmax(logits)\n",
        "    \n",
        "    # Cross entropy\n",
        "    # This quantifies how far off the predictions were.\n",
        "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction),reduction_indices = 1)\n",
        "    \n",
        "    # Training loss\n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    \n",
        "    # Rate at which the weights are changed\n",
        "    learning_rate = 0.08\n",
        "    \n",
        "    # Gradient Descent\n",
        "    # This is the method used to train the model\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "    \n",
        "    # Run optimizer and get loss\n",
        "    a, l = session.run(\n",
        "    [optimizer, loss],\n",
        "    feed_dict={features: train_features, labels: train_labels})\n",
        "    \n",
        "# print loss\n",
        "print('Loss: {}'.format(l))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 8.547343254089355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C_SVxTEe8bV",
        "colab_type": "text"
      },
      "source": [
        "# Quiz - Tensorflow softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIfRX2UJwP9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run():\n",
        "    output = None\n",
        "    logit_data = [2.0, 1.0, 0.1]\n",
        "    logits = tf.placeholder(tf.float32)\n",
        "    softmax = tf.nn.softmax(logits)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGjOavNowPOp",
        "colab_type": "text"
      },
      "source": [
        "# Quiz - Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op8cdLdnwO7q",
        "colab_type": "code",
        "outputId": "5a842d12-3918-471c-9989-23b1a9c25486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "softmax_data = [0.7,0.2,0.1]\n",
        "one_hot_data = [1.0,0.0,0.0]\n",
        "\n",
        "softmax = tf.placeholder(tf.float32)\n",
        "one_hot = tf.placeholder(tf.float32)\n",
        "\n",
        "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    output = sess.run(cross_entropy, feed_dict={one_hot: one_hot_data,\n",
        "                                           softmax: softmax_data})\n",
        "    print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35667497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1Xo7Ur9wOpZ",
        "colab_type": "text"
      },
      "source": [
        "# Quiz - Mini-batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgbzL9vVwOPR",
        "colab_type": "text"
      },
      "source": [
        "- first divide your data into batches. 但是并不是所有情况都能够等分数据集, 例如1000个数据点, 分成128份, 不能等分, 此时就需要`tf.placeholder()`, 其可以接受可变化的批次大小\n",
        "\n",
        "Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes].\n",
        "\n",
        "```\n",
        "# Features and Labels\n",
        "features = tf.placeholder(tf.float32, [None, n_input])\n",
        "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "What does None do here?\n",
        "\n",
        "The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbC9A9mUNhiM",
        "colab_type": "code",
        "outputId": "9567b87b-abab-42df-dd7a-8007475c6fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import math\n",
        "from pprint import pprint\n",
        "def batches(batch_size, features, labels):\n",
        "    \"\"\"\n",
        "    Create batches of features and labels\n",
        "    :param batch_size: The batch size\n",
        "    :param features: List of features\n",
        "    :param labels: List of labels\n",
        "    :return: Batches of (Features, Labels)\n",
        "    \"\"\"\n",
        "    assert len(features) == len(labels)\n",
        "    sample_size = len(features)\n",
        "    output_batches = []\n",
        "    for start_i in range(0, sample_size, batch_size):\n",
        "        end_i = start_i + batch_size\n",
        "        batch = [features[start_i:end_i], labels[start_i: end_i]]\n",
        "        output_batches.append(batch)\n",
        "    return output_batches    \n",
        "           \n",
        "    \n",
        "\n",
        "\n",
        "# 4 Samples of features\n",
        "example_features = [\n",
        "    ['F11','F12','F13','F14'],\n",
        "    ['F21','F22','F23','F24'],\n",
        "    ['F31','F32','F33','F34'],\n",
        "    ['F41','F42','F43','F44']]\n",
        "# 4 Samples of labels\n",
        "example_labels = [\n",
        "    ['L11','L12'],\n",
        "    ['L21','L22'],\n",
        "    ['L31','L32'],\n",
        "    ['L41','L42']]\n",
        "\n",
        "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
        "pprint(batches(3, example_features, example_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[['F11', 'F12', 'F13', 'F14'],\n",
            "   ['F21', 'F22', 'F23', 'F24'],\n",
            "   ['F31', 'F32', 'F33', 'F34']],\n",
            "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
            " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYixIST9NhKd",
        "colab_type": "code",
        "outputId": "b70e4e5e-efd4-49c7-da40-512ea1c9972d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.001\n",
        "n_input = 784  # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10  # MNIST total classes (0-9 digits)\n",
        "\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
        "\n",
        "# The features are already scaled and the data is shuffled\n",
        "train_features = mnist.train.images\n",
        "test_features = mnist.test.images\n",
        "\n",
        "train_labels = mnist.train.labels.astype(np.float32)\n",
        "test_labels = mnist.test.labels.astype(np.float32)\\\n",
        "\n",
        " # Features and Labels\n",
        "features = tf.placeholder(tf.float32, [None, n_input])\n",
        "labels = tf.placeholder(tf.float32, [None,n_classes])\n",
        "\n",
        "# Weights and bias\n",
        "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
        "bias = tf.Variable(tf.random_normal([n_classes]))\n",
        "\n",
        "# Logits - xW + b\n",
        "logits = tf.add(tf.matmul(features, weights), bias)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "batch_size = 128\n",
        "assert batch_size is not None, 'You must set the batch size'\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
        "        sess.run(optimizer, feed_dict={features:batch_features, \n",
        "                                        labels:batch_labels})\n",
        "        \n",
        "    test_accuracy = sess.run(\n",
        "    accuracy,\n",
        "    feed_dict={features: test_features, labels: test_labels})\n",
        "print('Test Accuracy: {}'.format(test_accuracy))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Test Accuracy: 0.10890000313520432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ9S7VjyNgmR",
        "colab_type": "text"
      },
      "source": [
        "# Quiz - Epochs\n",
        "\n",
        "used to increase the accuracy of the model without requiring more data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTDmQxR8Nd1a",
        "colab_type": "code",
        "outputId": "3bc59d69-9e4a-4a78-cde2-f1190f8074c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
        "    \"\"\"\n",
        "    Print cost and validation accuracy of an epoch\n",
        "    \"\"\"\n",
        "    current_cost = sess.run(\n",
        "        cost,\n",
        "        feed_dict={features:last_features, labels:last_labels})\n",
        "    valid_accuracy = sess.run(\n",
        "        accuracy,\n",
        "        feed_dict={features:last_features, labels:last_labels})\n",
        "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
        "        epoch_i,\n",
        "        current_cost,\n",
        "        valid_accuracy))\n",
        "    \n",
        "n_input = 784\n",
        "n_classes = 10\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
        "\n",
        "# The features are already scaled and the data is shuffled\n",
        "train_features = mnist.train.images\n",
        "valid_features = mnist.validation.images\n",
        "test_features = mnist.test.images\n",
        "\n",
        "train_labels = mnist.train.labels.astype(np.float32)\n",
        "valid_labels = mnist.validation.labels.astype(np.float32)\n",
        "test_labels = mnist.test.labels.astype(np.float32)\n",
        "\n",
        "# Features and Labels\n",
        "features = tf.placeholder(tf.float32, [None, n_input])\n",
        "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "\n",
        "# Weights & bias\n",
        "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
        "bias = tf.Variable(tf.random_normal([n_classes]))\n",
        "\n",
        "# Logits - xW + b\n",
        "logits = tf.add(tf.matmul(features, weights), bias)\n",
        "\n",
        "# Define loss and optimizer\n",
        "learning_rate = tf.placeholder(tf.float32)\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "learn_rate = 0.01\n",
        "\n",
        "train_batches = batches(batch_size, train_features, train_labels)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch_i in range(epochs):\n",
        "        for batch_features, batch_labels in train_batches:\n",
        "            train_feed_dict = {\n",
        "                features: batch_features,\n",
        "                labels: batch_labels,\n",
        "                learning_rate: learn_rate\n",
        "            }\n",
        "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
        "        # Print cost and validation accuracy of an epoch\n",
        "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
        "    # Calculate accuracy for test dataset\n",
        "    test_accuracy = sess.run(\n",
        "        accuracy,\n",
        "        feed_dict={features:test_features, labels:test_labels})\n",
        "print('Test Accuracy: {}'.format(test_accuracy))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0    - Cost: 7.76     Valid Accuracy: 0.136\n",
            "Epoch: 1    - Cost: 5.21     Valid Accuracy: 0.284\n",
            "Epoch: 2    - Cost: 3.87     Valid Accuracy: 0.341\n",
            "Epoch: 3    - Cost: 3.09     Valid Accuracy: 0.409\n",
            "Epoch: 4    - Cost: 2.62     Valid Accuracy: 0.5  \n",
            "Epoch: 5    - Cost: 2.28     Valid Accuracy: 0.523\n",
            "Epoch: 6    - Cost: 2.03     Valid Accuracy: 0.58 \n",
            "Epoch: 7    - Cost: 1.84     Valid Accuracy: 0.614\n",
            "Epoch: 8    - Cost: 1.68     Valid Accuracy: 0.636\n",
            "Epoch: 9    - Cost: 1.56     Valid Accuracy: 0.648\n",
            "Epoch: 10   - Cost: 1.45     Valid Accuracy: 0.659\n",
            "Epoch: 11   - Cost: 1.36     Valid Accuracy: 0.67 \n",
            "Epoch: 12   - Cost: 1.29     Valid Accuracy: 0.682\n",
            "Epoch: 13   - Cost: 1.22     Valid Accuracy: 0.693\n",
            "Epoch: 14   - Cost: 1.17     Valid Accuracy: 0.705\n",
            "Epoch: 15   - Cost: 1.12     Valid Accuracy: 0.727\n",
            "Epoch: 16   - Cost: 1.08     Valid Accuracy: 0.727\n",
            "Epoch: 17   - Cost: 1.04     Valid Accuracy: 0.75 \n",
            "Epoch: 18   - Cost: 1.01     Valid Accuracy: 0.761\n",
            "Epoch: 19   - Cost: 0.982    Valid Accuracy: 0.773\n",
            "Epoch: 20   - Cost: 0.956    Valid Accuracy: 0.795\n",
            "Epoch: 21   - Cost: 0.934    Valid Accuracy: 0.807\n",
            "Epoch: 22   - Cost: 0.913    Valid Accuracy: 0.807\n",
            "Epoch: 23   - Cost: 0.895    Valid Accuracy: 0.807\n",
            "Epoch: 24   - Cost: 0.878    Valid Accuracy: 0.807\n",
            "Epoch: 25   - Cost: 0.863    Valid Accuracy: 0.807\n",
            "Epoch: 26   - Cost: 0.849    Valid Accuracy: 0.818\n",
            "Epoch: 27   - Cost: 0.836    Valid Accuracy: 0.818\n",
            "Epoch: 28   - Cost: 0.824    Valid Accuracy: 0.818\n",
            "Epoch: 29   - Cost: 0.812    Valid Accuracy: 0.818\n",
            "Epoch: 30   - Cost: 0.802    Valid Accuracy: 0.818\n",
            "Epoch: 31   - Cost: 0.792    Valid Accuracy: 0.818\n",
            "Epoch: 32   - Cost: 0.783    Valid Accuracy: 0.818\n",
            "Epoch: 33   - Cost: 0.774    Valid Accuracy: 0.818\n",
            "Epoch: 34   - Cost: 0.766    Valid Accuracy: 0.83 \n",
            "Epoch: 35   - Cost: 0.758    Valid Accuracy: 0.83 \n",
            "Epoch: 36   - Cost: 0.751    Valid Accuracy: 0.83 \n",
            "Epoch: 37   - Cost: 0.744    Valid Accuracy: 0.83 \n",
            "Epoch: 38   - Cost: 0.738    Valid Accuracy: 0.83 \n",
            "Epoch: 39   - Cost: 0.731    Valid Accuracy: 0.83 \n",
            "Epoch: 40   - Cost: 0.725    Valid Accuracy: 0.83 \n",
            "Epoch: 41   - Cost: 0.72     Valid Accuracy: 0.852\n",
            "Epoch: 42   - Cost: 0.714    Valid Accuracy: 0.852\n",
            "Epoch: 43   - Cost: 0.709    Valid Accuracy: 0.864\n",
            "Epoch: 44   - Cost: 0.704    Valid Accuracy: 0.864\n",
            "Epoch: 45   - Cost: 0.7      Valid Accuracy: 0.864\n",
            "Epoch: 46   - Cost: 0.695    Valid Accuracy: 0.864\n",
            "Epoch: 47   - Cost: 0.691    Valid Accuracy: 0.864\n",
            "Epoch: 48   - Cost: 0.687    Valid Accuracy: 0.864\n",
            "Epoch: 49   - Cost: 0.683    Valid Accuracy: 0.864\n",
            "Epoch: 50   - Cost: 0.679    Valid Accuracy: 0.864\n",
            "Epoch: 51   - Cost: 0.675    Valid Accuracy: 0.864\n",
            "Epoch: 52   - Cost: 0.671    Valid Accuracy: 0.864\n",
            "Epoch: 53   - Cost: 0.668    Valid Accuracy: 0.864\n",
            "Epoch: 54   - Cost: 0.665    Valid Accuracy: 0.864\n",
            "Epoch: 55   - Cost: 0.661    Valid Accuracy: 0.864\n",
            "Epoch: 56   - Cost: 0.658    Valid Accuracy: 0.864\n",
            "Epoch: 57   - Cost: 0.655    Valid Accuracy: 0.864\n",
            "Epoch: 58   - Cost: 0.652    Valid Accuracy: 0.864\n",
            "Epoch: 59   - Cost: 0.649    Valid Accuracy: 0.864\n",
            "Epoch: 60   - Cost: 0.647    Valid Accuracy: 0.864\n",
            "Epoch: 61   - Cost: 0.644    Valid Accuracy: 0.864\n",
            "Epoch: 62   - Cost: 0.641    Valid Accuracy: 0.864\n",
            "Epoch: 63   - Cost: 0.639    Valid Accuracy: 0.875\n",
            "Epoch: 64   - Cost: 0.636    Valid Accuracy: 0.875\n",
            "Epoch: 65   - Cost: 0.633    Valid Accuracy: 0.875\n",
            "Epoch: 66   - Cost: 0.631    Valid Accuracy: 0.886\n",
            "Epoch: 67   - Cost: 0.629    Valid Accuracy: 0.886\n",
            "Epoch: 68   - Cost: 0.626    Valid Accuracy: 0.886\n",
            "Epoch: 69   - Cost: 0.624    Valid Accuracy: 0.886\n",
            "Epoch: 70   - Cost: 0.622    Valid Accuracy: 0.886\n",
            "Epoch: 71   - Cost: 0.619    Valid Accuracy: 0.886\n",
            "Epoch: 72   - Cost: 0.617    Valid Accuracy: 0.886\n",
            "Epoch: 73   - Cost: 0.615    Valid Accuracy: 0.886\n",
            "Epoch: 74   - Cost: 0.613    Valid Accuracy: 0.886\n",
            "Epoch: 75   - Cost: 0.611    Valid Accuracy: 0.886\n",
            "Epoch: 76   - Cost: 0.609    Valid Accuracy: 0.886\n",
            "Epoch: 77   - Cost: 0.607    Valid Accuracy: 0.886\n",
            "Epoch: 78   - Cost: 0.605    Valid Accuracy: 0.886\n",
            "Epoch: 79   - Cost: 0.603    Valid Accuracy: 0.886\n",
            "Epoch: 80   - Cost: 0.601    Valid Accuracy: 0.886\n",
            "Epoch: 81   - Cost: 0.599    Valid Accuracy: 0.886\n",
            "Epoch: 82   - Cost: 0.597    Valid Accuracy: 0.886\n",
            "Epoch: 83   - Cost: 0.595    Valid Accuracy: 0.886\n",
            "Epoch: 84   - Cost: 0.593    Valid Accuracy: 0.886\n",
            "Epoch: 85   - Cost: 0.591    Valid Accuracy: 0.886\n",
            "Epoch: 86   - Cost: 0.59     Valid Accuracy: 0.886\n",
            "Epoch: 87   - Cost: 0.588    Valid Accuracy: 0.886\n",
            "Epoch: 88   - Cost: 0.586    Valid Accuracy: 0.886\n",
            "Epoch: 89   - Cost: 0.584    Valid Accuracy: 0.886\n",
            "Epoch: 90   - Cost: 0.583    Valid Accuracy: 0.886\n",
            "Epoch: 91   - Cost: 0.581    Valid Accuracy: 0.886\n",
            "Epoch: 92   - Cost: 0.579    Valid Accuracy: 0.886\n",
            "Epoch: 93   - Cost: 0.578    Valid Accuracy: 0.886\n",
            "Epoch: 94   - Cost: 0.576    Valid Accuracy: 0.886\n",
            "Epoch: 95   - Cost: 0.574    Valid Accuracy: 0.886\n",
            "Epoch: 96   - Cost: 0.573    Valid Accuracy: 0.886\n",
            "Epoch: 97   - Cost: 0.571    Valid Accuracy: 0.886\n",
            "Epoch: 98   - Cost: 0.57     Valid Accuracy: 0.886\n",
            "Epoch: 99   - Cost: 0.568    Valid Accuracy: 0.886\n",
            "Test Accuracy: 0.8763999938964844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4AfgP3Tpvwi",
        "colab_type": "text"
      },
      "source": [
        "<h1 align=\"center\">TensorFlow Neural Network Lab</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8wkDG_npvwk",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"image/notmnist.png\">\n",
        "In this lab, you'll use all the tools you learned from *Introduction to TensorFlow* to label images of English letters! The data you are using, <a href=\"http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\">notMNIST</a>, consists of images of a letter from A to J in different fonts.\n",
        "\n",
        "The above images are a few examples of the data you'll be training on. After training the network, you will compare your prediction model against test data. Your goal, by the end of this lab, is to make predictions against that test set with at least an 80% accuracy. Let's jump in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41bPASFcpvwk",
        "colab_type": "text"
      },
      "source": [
        "To start this lab, you first need to import all the necessary modules. Run the code below. If it runs successfully, it will print \"`All modules imported`\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXRB5LyTpvwl",
        "colab_type": "code",
        "outputId": "ff619913-02c5-49e8-aabc-b9f221491a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import hashlib\n",
        "import os\n",
        "import pickle\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import resample\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print('All modules imported.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All modules imported.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwzFjv1Ppvwp",
        "colab_type": "text"
      },
      "source": [
        "The notMNIST dataset is too large for many computers to handle.  It contains 500,000 images for just training.  You'll be using a subset of this data, 15,000 images for each label (A-J)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybbpMzcapvwq",
        "colab_type": "code",
        "outputId": "4af0182a-fee1-4f49-e9ad-113b1ca349b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def download(url, file):\n",
        "    \"\"\"\n",
        "    Download file from <url>\n",
        "    :param url: URL to file\n",
        "    :param file: Local file path\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(file):\n",
        "        print('Downloading ' + file + '...')\n",
        "        urlretrieve(url, file)\n",
        "        print('Download Finished')\n",
        "\n",
        "# Download the training and test dataset.\n",
        "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')\n",
        "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')\n",
        "\n",
        "# Make sure the files aren't corrupted\n",
        "assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\\\n",
        "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
        "assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\\\n",
        "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'\n",
        "\n",
        "# Wait until you see that all files have been downloaded.\n",
        "print('All files downloaded.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All files downloaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWVijs0opvws",
        "colab_type": "code",
        "outputId": "29739fb0-3d19-4dae-9368-a8e413353614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "def uncompress_features_labels(file):\n",
        "    \"\"\"\n",
        "    Uncompress features and labels from a zip file\n",
        "    :param file: The zip file to extract the data from\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with ZipFile(file) as zipf:\n",
        "        # Progress Bar\n",
        "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
        "        \n",
        "        # Get features and labels from all files\n",
        "        for filename in filenames_pbar:\n",
        "            # Check if the file is a directory\n",
        "            if not filename.endswith('/'):\n",
        "                with zipf.open(filename) as image_file:\n",
        "                    image = Image.open(image_file)\n",
        "                    image.load()\n",
        "                    # Load image data as 1 dimensional array\n",
        "                    # We're using float32 to save on memory space\n",
        "                    feature = np.array(image, dtype=np.float32).flatten()\n",
        "\n",
        "                # Get the the letter from the filename.  This is the letter of the image.\n",
        "                label = os.path.split(filename)[1][0]\n",
        "\n",
        "                features.append(feature)\n",
        "                labels.append(label)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Get the features and labels from the zip files\n",
        "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
        "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
        "\n",
        "# Limit the amount of data to work with a docker container\n",
        "docker_size_limit = 150000\n",
        "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
        "\n",
        "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
        "is_features_normal = False\n",
        "is_labels_encod = False\n",
        "\n",
        "# Wait until you see that all features and labels have been uncompressed.\n",
        "print('All features and labels uncompressed.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 210001/210001 [00:27<00:00, 7512.22files/s]\n",
            "100%|██████████| 10001/10001 [00:01<00:00, 7687.84files/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All features and labels uncompressed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OctvaZnBpvwv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Problem 1\n",
        "第一个问题涉及标准化训练和测试数据。\n",
        "\n",
        "\n",
        "Implement Min-Max scaling in the `normalize_grayscale()` function to a range of `a=0.1` and `b=0.9`. After scaling, the values of the pixels in the input data should range from 0.1 to 0.9.\n",
        "\n",
        "Since the raw notMNIST image data is in [grayscale](https://en.wikipedia.org/wiki/Grayscale), the current values range from a min of 0 to a max of 255.\n",
        "\n",
        "Min-Max Scaling:\n",
        "$\n",
        "X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}\n",
        "$\n",
        "\n",
        "*If you're having trouble solving problem 1, you can view the solution [here](https://github.com/udacity/deep-learning/blob/master/intro-to-tensorflow/intro_to_tensorflow_solution.ipynb).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2MixRRgoyhd",
        "colab_type": "code",
        "outputId": "86f3ee4e-cb22-470b-9bf4-493a4f51d4db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.max([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sILgkWAhpvww",
        "colab_type": "code",
        "outputId": "9aeaf975-3f1f-4795-c6c7-258aa1603cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
        "def normalize_grayscale(image_data):\n",
        "    \"\"\"\n",
        "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
        "    :param image_data: The image data to be normalized\n",
        "    :return: Normalized image data\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: Implement Min-Max scaling for grayscale image data\n",
        "    x_max = 255\n",
        "    x_min = 0\n",
        "    a = 0.1\n",
        "    b = 0.9\n",
        "    return (image_data-x_min)*(b-a)/(x_max-x_min) + a\n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "# Test Cases\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),\n",
        "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
        "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
        "    decimal=3)\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254,255])),\n",
        "    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,\n",
        "     0.896862745098, 0.9])\n",
        "\n",
        "if not is_features_normal:\n",
        "    train_features = normalize_grayscale(train_features)\n",
        "    test_features = normalize_grayscale(test_features)\n",
        "    is_features_normal = True\n",
        "\n",
        "print('Tests Passed!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXSEu1Skpvwy",
        "colab_type": "code",
        "outputId": "dfa43b64-20bc-4351-d02f-622c8b30d504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if not is_labels_encod:\n",
        "    # Turn labels into numbers and apply One-Hot Encoding\n",
        "    encoder = LabelBinarizer()\n",
        "    encoder.fit(train_labels)\n",
        "    train_labels = encoder.transform(train_labels)\n",
        "    test_labels = encoder.transform(test_labels)\n",
        "\n",
        "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
        "    train_labels = train_labels.astype(np.float32)\n",
        "    test_labels = test_labels.astype(np.float32)\n",
        "    is_labels_encod = True\n",
        "\n",
        "print('Labels One-Hot Encoded')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels One-Hot Encoded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZJoTZagpvw1",
        "colab_type": "code",
        "outputId": "d35ed636-eb66-4136-edb6-5148596c9e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "assert is_features_normal, 'You skipped the step to normalize the features'\n",
        "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
        "\n",
        "# Get randomized datasets for training and validation\n",
        "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    test_size=0.05,\n",
        "    random_state=832289)\n",
        "\n",
        "print('Training features and labels randomized and split.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training features and labels randomized and split.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBRmHC72pvw3",
        "colab_type": "code",
        "outputId": "a1b1bb1c-c294-4619-daed-79555f00e87f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Save the data for easy access\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "if not os.path.isfile(pickle_file):\n",
        "    print('Saving data to pickle file...')\n",
        "    try:\n",
        "        with open('notMNIST.pickle', 'wb') as pfile:\n",
        "            pickle.dump(\n",
        "                {\n",
        "                    'train_dataset': train_features,\n",
        "                    'train_labels': train_labels,\n",
        "                    'valid_dataset': valid_features,\n",
        "                    'valid_labels': valid_labels,\n",
        "                    'test_dataset': test_features,\n",
        "                    'test_labels': test_labels,\n",
        "                },\n",
        "                pfile, pickle.HIGHEST_PROTOCOL)\n",
        "    except Exception as e:\n",
        "        print('Unable to save data to', pickle_file, ':', e)\n",
        "        raise\n",
        "\n",
        "print('Data cached in pickle file.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving data to pickle file...\n",
            "Data cached in pickle file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaApd5eOpvw6",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoint\n",
        "All your progress is now saved to the pickle file.  If you need to leave and comeback to this lab, you no longer have to start from the beginning.  Just run the code block below and it will load all the data and modules required to proceed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5JtmZCNpvw7",
        "colab_type": "code",
        "outputId": "61da43e1-09aa-4ad6-cf3a-ff7804807e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Load the modules\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reload the data\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "with open(pickle_file, 'rb') as f:\n",
        "    pickle_data = pickle.load(f)\n",
        "    train_features = pickle_data['train_dataset']\n",
        "    train_labels = pickle_data['train_labels']\n",
        "    valid_features = pickle_data['valid_dataset']\n",
        "    valid_labels = pickle_data['valid_labels']\n",
        "    test_features = pickle_data['test_dataset']\n",
        "    test_labels = pickle_data['test_labels']\n",
        "    del pickle_data  # Free up memory\n",
        "\n",
        "print('Data and modules loaded.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data and modules loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si2SHEvVpvw_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Problem 2\n",
        "\n",
        "Now it's time to build a simple neural network using TensorFlow. Here, your network will be just an input layer and an output layer.\n",
        "\n",
        "\n",
        "\n",
        "For the input here the images have been flattened into a vector of $28 \\times 28 = 784$ features. Then, we're trying to predict the image digit so there are 10 output units, one for each label. Of course, feel free to add hidden layers if you want, but this notebook is built to guide you through a single layer network. \n",
        "\n",
        "For the neural network to train on your data, you need the following <a href=\"https://www.tensorflow.org/resources/dims_types.html#data-types\">float32</a> tensors:\n",
        " - `features`\n",
        "  - Placeholder tensor for feature data (`train_features`/`valid_features`/`test_features`)\n",
        " - `labels`\n",
        "  - Placeholder tensor for label data (`train_labels`/`valid_labels`/`test_labels`)\n",
        " - `weights`\n",
        "  - Variable Tensor with random numbers from a truncated normal distribution.\n",
        "    - See <a href=\"https://www.tensorflow.org/api_docs/python/constant_op.html#truncated_normal\">`tf.truncated_normal()` documentation</a> for help.\n",
        " - `biases`\n",
        "  - Variable Tensor with all zeros.\n",
        "    - See <a href=\"https://www.tensorflow.org/api_docs/python/constant_op.html#zeros\"> `tf.zeros()` documentation</a> for help.\n",
        "\n",
        "*If you're having trouble solving problem 2, review \"TensorFlow Linear Function\" section of the class.  If that doesn't help, the solution for this problem is available [here](intro_to_tensorflow_solution.ipynb).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-S5ik0upvw_",
        "colab_type": "code",
        "outputId": "8c04d4db-b8fa-4009-8d90-c32347ae8e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# All the pixels in the image (28 * 28 = 784)\n",
        "features_count = 784\n",
        "# All the labels\n",
        "labels_count = 10\n",
        "\n",
        "# TODO: Set the features and labels tensors\n",
        "features = tf.placeholder(tf.float32, [None, features_count])\n",
        "labels = tf.placeholder(tf.float32, [None, labels_count])\n",
        "\n",
        "# TODO: Set the weights and biases tensors\n",
        "weights = tf.Variable(tf.truncated_normal([features_count, labels_count]))\n",
        "biases = tf.Variable(tf.zeros([labels_count]))\n",
        "\n",
        "\n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "\n",
        "#Test Cases\n",
        "from tensorflow.python.ops.variables import Variable\n",
        "\n",
        "assert features._op.name.startswith('Placeholder'), 'features must be a placeholder'\n",
        "assert labels._op.name.startswith('Placeholder'), 'labels must be a placeholder'\n",
        "assert isinstance(weights, Variable), 'weights must be a TensorFlow variable'\n",
        "assert isinstance(biases, Variable), 'biases must be a TensorFlow variable'\n",
        "\n",
        "assert features._shape == None or (\\\n",
        "    features._shape.dims[0].value is None and\\\n",
        "    features._shape.dims[1].value in [None, 784]), 'The shape of features is incorrect'\n",
        "assert labels._shape  == None or (\\\n",
        "    labels._shape.dims[0].value is None and\\\n",
        "    labels._shape.dims[1].value in [None, 10]), 'The shape of labels is incorrect'\n",
        "assert weights._variable._shape == (784, 10), 'The shape of weights is incorrect'\n",
        "assert biases._variable._shape == (10), 'The shape of biases is incorrect'\n",
        "\n",
        "assert features._dtype == tf.float32, 'features must be type float32'\n",
        "assert labels._dtype == tf.float32, 'labels must be type float32'\n",
        "\n",
        "# Feed dicts for training, validation, and test session\n",
        "train_feed_dict = {features: train_features, labels: train_labels}\n",
        "valid_feed_dict = {features: valid_features, labels: valid_labels}\n",
        "test_feed_dict = {features: test_features, labels: test_labels}\n",
        "\n",
        "# Linear Function WX + b\n",
        "logits = tf.matmul(features, weights) + biases\n",
        "\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Cross entropy\n",
        "cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
        "\n",
        "# Training loss\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "# Create an operation that initializes all variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Test Cases\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    session.run(loss, feed_dict=train_feed_dict)\n",
        "    session.run(loss, feed_dict=valid_feed_dict)\n",
        "    session.run(loss, feed_dict=test_feed_dict)\n",
        "    biases_data = session.run(biases)\n",
        "\n",
        "assert not np.count_nonzero(biases_data), 'biases must be zeros'\n",
        "\n",
        "print('Tests Passed!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0714 13:32:25.642801 140694784546688 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "W0714 13:32:25.645797 140694784546688 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "W0714 13:32:25.646819 140694784546688 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "W0714 13:32:25.649455 140694784546688 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "W0714 13:32:25.650774 140694784546688 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "W0714 13:32:25.651564 140694784546688 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "W0714 13:32:25.652220 140694784546688 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
            "W0714 13:32:25.653340 140694784546688 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tests Passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCS9gFTJpvxB",
        "colab_type": "code",
        "outputId": "f49be67a-c63b-44ad-8413-f64769d12c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Determine if the predictions are correct\n",
        "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
        "# Calculate the accuracy of the predictions\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
        "\n",
        "print('Accuracy function created.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy function created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E99-0OwJpvxD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Problem 3\n",
        "Below are 2 parameter configurations for training the neural network. In each configuration, one of the parameters has multiple options. For each configuration, choose the option that gives the best acccuracy.\n",
        "\n",
        "Parameter configurations:\n",
        "\n",
        "Configuration 1\n",
        "* **Epochs:** 1\n",
        "* **Learning Rate:**\n",
        "  * 0.8\n",
        "  * 0.5\n",
        "  * 0.1\n",
        "  * 0.05\n",
        "  * 0.01\n",
        "\n",
        "Configuration 2\n",
        "* **Epochs:**\n",
        "  * 1\n",
        "  * 2\n",
        "  * 3\n",
        "  * 4\n",
        "  * 5\n",
        "* **Learning Rate:** 0.2\n",
        "\n",
        "The code will print out a Loss and Accuracy graph, so you can see how well the neural network performed.\n",
        "\n",
        "*If you're having trouble solving problem 3, you can view the solution [here](intro_to_tensorflow_solution.ipynb).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3f0U1k3pvxE",
        "colab_type": "code",
        "outputId": "5156f842-847e-4b33-900f-02ba6c14a172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "# Change if you have memory restrictions\n",
        "batch_size = 128\n",
        "\n",
        "# TODO: Find the best parameters for each configuration\n",
        "epochs = 5\n",
        "learning_rate = 0.2\n",
        "\n",
        "\n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "# Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n",
        "\n",
        "# The accuracy measured against the validation set\n",
        "validation_accuracy = 0.0\n",
        "\n",
        "# Measurements use for graphing loss and accuracy\n",
        "log_batch_step = 50\n",
        "batches = []\n",
        "loss_batch = []\n",
        "train_acc_batch = []\n",
        "valid_acc_batch = []\n",
        "\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        \n",
        "        # Progress bar\n",
        "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
        "        \n",
        "        # The training cycle\n",
        "        for batch_i in batches_pbar:\n",
        "            # Get a batch of training features and labels\n",
        "            batch_start = batch_i*batch_size\n",
        "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
        "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "            # Run optimizer and get loss\n",
        "            _, l = session.run(\n",
        "                [optimizer, loss],\n",
        "                feed_dict={features: batch_features, labels: batch_labels})\n",
        "\n",
        "            # Log every 50 batches\n",
        "            if not batch_i % log_batch_step:\n",
        "                # Calculate Training and Validation accuracy\n",
        "                training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)\n",
        "                validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
        "\n",
        "                # Log batches\n",
        "                previous_batch = batches[-1] if batches else 0\n",
        "                batches.append(log_batch_step + previous_batch)\n",
        "                loss_batch.append(l)\n",
        "                train_acc_batch.append(training_accuracy)\n",
        "                valid_acc_batch.append(validation_accuracy)\n",
        "\n",
        "        # Check accuracy against Validation data\n",
        "        validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
        "\n",
        "loss_plot = plt.subplot(211)\n",
        "loss_plot.set_title('Loss')\n",
        "loss_plot.plot(batches, loss_batch, 'g')\n",
        "loss_plot.set_xlim([batches[0], batches[-1]])\n",
        "acc_plot = plt.subplot(212)\n",
        "acc_plot.set_title('Accuracy')\n",
        "acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')\n",
        "acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')\n",
        "acc_plot.set_ylim([0, 1.0])\n",
        "acc_plot.set_xlim([batches[0], batches[-1]])\n",
        "acc_plot.legend(loc=4)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Validation accuracy at {}'.format(validation_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1/5: 100%|██████████| 1114/1114 [00:07<00:00, 145.93batches/s]\n",
            "Epoch  2/5: 100%|██████████| 1114/1114 [00:07<00:00, 146.24batches/s]\n",
            "Epoch  3/5: 100%|██████████| 1114/1114 [00:07<00:00, 146.43batches/s]\n",
            "Epoch  4/5: 100%|██████████| 1114/1114 [00:07<00:00, 146.04batches/s]\n",
            "Epoch  5/5: 100%|██████████| 1114/1114 [00:07<00:00, 146.41batches/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy at 0.7802666425704956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "namgv42vpvxG",
        "colab_type": "text"
      },
      "source": [
        "## Test\n",
        "You're going to test your model against your hold out dataset/testing data.  This will give you a good indicator of how well the model will do in the real world.  You should have a test accuracy of at least 80%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXfzipZGpvxG",
        "colab_type": "code",
        "outputId": "4dab38ca-64a6-4696-dd02-210b9d537545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "# The accuracy measured against the test set\n",
        "test_accuracy = 0.0\n",
        "\n",
        "with tf.Session() as session:\n",
        "    \n",
        "    session.run(init)\n",
        "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        \n",
        "        # Progress bar\n",
        "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
        "        \n",
        "        # The training cycle\n",
        "        for batch_i in batches_pbar:\n",
        "            # Get a batch of training features and labels\n",
        "            batch_start = batch_i*batch_size\n",
        "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
        "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "            # Run optimizer\n",
        "            _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
        "\n",
        "        # Check accuracy against Test data\n",
        "        test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)\n",
        "\n",
        "\n",
        "assert test_accuracy >= 0.80, 'Test accuracy at {}, should be equal to or greater than 0.80'.format(test_accuracy)\n",
        "print('Nice Job! Test Accuracy is {}'.format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1/5: 100%|██████████| 1114/1114 [00:01<00:00, 695.09batches/s]\n",
            "Epoch  2/5: 100%|██████████| 1114/1114 [00:01<00:00, 787.37batches/s]\n",
            "Epoch  3/5: 100%|██████████| 1114/1114 [00:01<00:00, 809.07batches/s]\n",
            "Epoch  4/5: 100%|██████████| 1114/1114 [00:01<00:00, 806.06batches/s]\n",
            "Epoch  5/5: 100%|██████████| 1114/1114 [00:01<00:00, 805.41batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Nice Job! Test Accuracy is 0.8391000032424927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxH1dGXTpvxI",
        "colab_type": "text"
      },
      "source": [
        "# Multiple layers\n",
        "Good job!  You built a one layer TensorFlow network!  However, you might want to build more than one layer.  This is deep learning after all!  In the next section, you will start to satisfy your need for more layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88c70zEyv-Zz",
        "colab_type": "text"
      },
      "source": [
        "# Quiz - TensorFlow ReLUs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sWxkQ5Dx4MB",
        "colab_type": "code",
        "outputId": "b9d15409-3946-4dba-8734-7c2365d962d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "output = None\n",
        "hidden_layer_weights = [\n",
        "    [0.1, 0.2, 0.4],\n",
        "    [0.4, 0.6, 0.6],\n",
        "    [0.5, 0.9, 0.1],\n",
        "    [0.8, 0.2, 0.8]]\n",
        "out_weights = [\n",
        "    [0.1, 0.6],\n",
        "    [0.2, 0.1],\n",
        "    [0.7, 0.9]]\n",
        "\n",
        "# Weights and biases\n",
        "weights = [\n",
        "    tf.Variable(hidden_layer_weights),\n",
        "    tf.Variable(out_weights)]\n",
        "biases = [\n",
        "    tf.Variable(tf.zeros(3)),\n",
        "    tf.Variable(tf.zeros(2))]\n",
        "\n",
        "# Input\n",
        "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
        "\n",
        "# TODO: Create Model\n",
        "hidden_layer = tf.nn.relu(tf.add(tf.matmul(features, weights[0]), biases[0]))\n",
        "logits = tf.add(tf.matmul(hidden_output, weights[1]), biases[1])\n",
        "\n",
        "# TODO: Print session results\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    print(sess.run(logits))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5.1099997  8.44     ]\n",
            " [ 0.         0.       ]\n",
            " [24.010002  38.24     ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKzMn3gl0F-V",
        "colab_type": "text"
      },
      "source": [
        "# Deep Neural Network in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M4pKVsp1gq3",
        "colab_type": "code",
        "outputId": "8d5f36a9-d311-4b66-908d-1acd74449dbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 20\n",
        "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
        "display_step = 1\n",
        "\n",
        "n_input = 784  # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10  # MNIST total classes (0-9 digits)\n",
        "\n",
        "n_hidden_layer = 256 # layer number of features\n",
        "\n",
        "weights = {\n",
        "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, 28,28, 1])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "x_flat = tf.reshape(x, [-1, n_input])\n",
        "\n",
        "# Hidden layer with RELU activation\n",
        "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
        "layer_1 = tf.nn.relu(layer_1)\n",
        "# Output layer with linear activation\n",
        "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        \n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op(backprop) and cost op(to get loss value)\n",
        "            sess.run(optimizer, feed_dict={x:batch_x, y:batch_y})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting ./train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting ./train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUbN4er03Tqw",
        "colab_type": "text"
      },
      "source": [
        "#  Save and Restore TensorFlow Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMccFFaBE6s7",
        "colab_type": "text"
      },
      "source": [
        "Training a model can take hours. But once you close your TensorFlow session, you lose all the trained weights and biases. If you were to reuse the model in the future, you would have to train it all over again!\n",
        "\n",
        "Fortunately, TensorFlow gives you the ability to save your progress using a class called tf.train.Saver. This class provides the functionality to save any tf.Variable to your file system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn13cSXLFJUy",
        "colab_type": "text"
      },
      "source": [
        "##  Saving Variables\n",
        "\n",
        "Let's start with a simple example of saving weights and bias Tensors. For the first example you'll just save two variables. Later examples will save all the weights in a practical model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlZnkYneGQRD",
        "colab_type": "code",
        "outputId": "6ddee94c-5388-490d-a6b2-544a873af10b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# The file path to save the data\n",
        "save_file = './model.ckpt'\n",
        "\n",
        "# Two Tensor Variables: weights and bias\n",
        "weights = tf.Variable(tf.truncated_normal([2,3]))\n",
        "bias = tf.Variable(tf.truncated_normal([3]))\n",
        "\n",
        "# Class used to save and/or restore Tensor Variables\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize all the Variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Show the value of weights and bias\n",
        "    print('Weights:')\n",
        "    print(sess.run(weights))\n",
        "    print('Bias:')\n",
        "    print(sess.run(bias))\n",
        "    \n",
        "    # save the model\n",
        "    saver.save(sess, save_file)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights:\n",
            "[[ 0.53742594 -0.12322772 -0.16663451]\n",
            " [-0.90039957  0.97000945  0.9874395 ]]\n",
            "Bias:\n",
            "[-0.77557915 -0.26914498 -0.08074012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iqdbLC1FUcK",
        "colab_type": "text"
      },
      "source": [
        "## Loading Variables\n",
        "\n",
        "Now that the Tensor Variables are saved, let's load them back into a new model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJknX_4ZHpYO",
        "colab_type": "code",
        "outputId": "5012caaa-2275-49cf-b276-c48eab3ec82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# Remove the previous weights and bias\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Two Variables: weights and bias\n",
        "weights = tf.Variable(tf.truncated_normal([2,3]))\n",
        "bias = tf.Variable(tf.truncated_normal([3]))\n",
        "\n",
        "# Class used to save and/or restore Tensor Variables\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Load the weights and bias\n",
        "    saver.restore(sess, save_file)\n",
        "    \n",
        "    # Show the value of weights and bias\n",
        "    print('Weights:')\n",
        "    print(sess.run(weights))\n",
        "    print('Bias:')\n",
        "    print(sess.run(bias))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0714 15:38:02.490671 140054717048704 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Weights:\n",
            "[[ 0.53742594 -0.12322772 -0.16663451]\n",
            " [-0.90039957  0.97000945  0.9874395 ]]\n",
            "Bias:\n",
            "[-0.77557915 -0.26914498 -0.08074012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWUJA4ovFWcy",
        "colab_type": "text"
      },
      "source": [
        "## Save a Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm_XGbfMIs-a",
        "colab_type": "code",
        "outputId": "2173ffda-9e43-4ce8-ad2f-f12230b69d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Remove previous Tensors and Operations\n",
        "tf.reset_default_graph()\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.001\n",
        "n_input = 784  # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10  # MNIST total classes (0-9 digits)\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = input_data.read_data_sets('.', one_hot=True)\n",
        "\n",
        "# Features and Labels\n",
        "features = tf.placeholder(tf.float32, [None, n_input])\n",
        "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Weights & bias\n",
        "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
        "bias = tf.Variable(tf.random_normal([n_classes]))\n",
        "\n",
        "# Logits - xW + b\n",
        "logits = tf.add(tf.matmul(features, weights), bias)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "import math\n",
        "save_file = './train_model.ckpt'\n",
        "batch_size = 128\n",
        "n_epochs = 100\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
        "        \n",
        "        for i in range(total_batch):\n",
        "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
        "            sess.run(\n",
        "                optimizer,\n",
        "                feed_dict={features: batch_features, labels: batch_labels})\n",
        "            \n",
        "        # Print status for every 10 epochs\n",
        "        if epoch % 10:\n",
        "            valid_accuracy = sess.run(\n",
        "                accuracy,\n",
        "                feed_dict={\n",
        "                    features: mnist.validation.images,\n",
        "                    labels: mnist.validation.labels})\n",
        "            print('Epoch {:<3} - Validation Accuracy: {}'.format(\n",
        "                epoch,\n",
        "                valid_accuracy))\n",
        "    # Save the model\n",
        "    saver.save(sess, save_file)\n",
        "    print('Trained Model Saved')\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./train-images-idx3-ubyte.gz\n",
            "Extracting ./train-labels-idx1-ubyte.gz\n",
            "Extracting ./t10k-images-idx3-ubyte.gz\n",
            "Extracting ./t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0714 15:50:53.756414 140054717048704 deprecation.py:323] From <ipython-input-6-42114beaade6>:25: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1   - Validation Accuracy: 0.07999999821186066\n",
            "Epoch 2   - Validation Accuracy: 0.09300000220537186\n",
            "Epoch 3   - Validation Accuracy: 0.10819999873638153\n",
            "Epoch 4   - Validation Accuracy: 0.12800000607967377\n",
            "Epoch 5   - Validation Accuracy: 0.15240000188350677\n",
            "Epoch 6   - Validation Accuracy: 0.1728000044822693\n",
            "Epoch 7   - Validation Accuracy: 0.19499999284744263\n",
            "Epoch 8   - Validation Accuracy: 0.21660000085830688\n",
            "Epoch 9   - Validation Accuracy: 0.23739999532699585\n",
            "Epoch 11  - Validation Accuracy: 0.27219998836517334\n",
            "Epoch 12  - Validation Accuracy: 0.2856000065803528\n",
            "Epoch 13  - Validation Accuracy: 0.3001999855041504\n",
            "Epoch 14  - Validation Accuracy: 0.3156000077724457\n",
            "Epoch 15  - Validation Accuracy: 0.329800009727478\n",
            "Epoch 16  - Validation Accuracy: 0.34360000491142273\n",
            "Epoch 17  - Validation Accuracy: 0.3580000102519989\n",
            "Epoch 18  - Validation Accuracy: 0.37459999322891235\n",
            "Epoch 19  - Validation Accuracy: 0.38499999046325684\n",
            "Epoch 21  - Validation Accuracy: 0.41100001335144043\n",
            "Epoch 22  - Validation Accuracy: 0.4205999970436096\n",
            "Epoch 23  - Validation Accuracy: 0.4320000112056732\n",
            "Epoch 24  - Validation Accuracy: 0.444599986076355\n",
            "Epoch 25  - Validation Accuracy: 0.4546000063419342\n",
            "Epoch 26  - Validation Accuracy: 0.46459999680519104\n",
            "Epoch 27  - Validation Accuracy: 0.4747999906539917\n",
            "Epoch 28  - Validation Accuracy: 0.48399999737739563\n",
            "Epoch 29  - Validation Accuracy: 0.4909999966621399\n",
            "Epoch 31  - Validation Accuracy: 0.508400022983551\n",
            "Epoch 32  - Validation Accuracy: 0.5156000256538391\n",
            "Epoch 33  - Validation Accuracy: 0.5224000215530396\n",
            "Epoch 34  - Validation Accuracy: 0.5303999781608582\n",
            "Epoch 35  - Validation Accuracy: 0.5368000268936157\n",
            "Epoch 36  - Validation Accuracy: 0.5432000160217285\n",
            "Epoch 37  - Validation Accuracy: 0.5508000254631042\n",
            "Epoch 38  - Validation Accuracy: 0.5587999820709229\n",
            "Epoch 39  - Validation Accuracy: 0.5655999779701233\n",
            "Epoch 41  - Validation Accuracy: 0.5753999948501587\n",
            "Epoch 42  - Validation Accuracy: 0.5799999833106995\n",
            "Epoch 43  - Validation Accuracy: 0.5860000252723694\n",
            "Epoch 44  - Validation Accuracy: 0.5893999934196472\n",
            "Epoch 45  - Validation Accuracy: 0.5938000082969666\n",
            "Epoch 46  - Validation Accuracy: 0.5971999764442444\n",
            "Epoch 47  - Validation Accuracy: 0.602400004863739\n",
            "Epoch 48  - Validation Accuracy: 0.6068000197410583\n",
            "Epoch 49  - Validation Accuracy: 0.6122000217437744\n",
            "Epoch 51  - Validation Accuracy: 0.6194000244140625\n",
            "Epoch 52  - Validation Accuracy: 0.625\n",
            "Epoch 53  - Validation Accuracy: 0.6284000277519226\n",
            "Epoch 54  - Validation Accuracy: 0.6326000094413757\n",
            "Epoch 55  - Validation Accuracy: 0.6348000168800354\n",
            "Epoch 56  - Validation Accuracy: 0.6371999979019165\n",
            "Epoch 57  - Validation Accuracy: 0.6402000188827515\n",
            "Epoch 58  - Validation Accuracy: 0.6434000134468079\n",
            "Epoch 59  - Validation Accuracy: 0.6453999876976013\n",
            "Epoch 61  - Validation Accuracy: 0.652400016784668\n",
            "Epoch 62  - Validation Accuracy: 0.6538000106811523\n",
            "Epoch 63  - Validation Accuracy: 0.6567999720573425\n",
            "Epoch 64  - Validation Accuracy: 0.6585999727249146\n",
            "Epoch 65  - Validation Accuracy: 0.6611999869346619\n",
            "Epoch 66  - Validation Accuracy: 0.6639999747276306\n",
            "Epoch 67  - Validation Accuracy: 0.6672000288963318\n",
            "Epoch 68  - Validation Accuracy: 0.6692000031471252\n",
            "Epoch 69  - Validation Accuracy: 0.6710000038146973\n",
            "Epoch 71  - Validation Accuracy: 0.6761999726295471\n",
            "Epoch 72  - Validation Accuracy: 0.6777999997138977\n",
            "Epoch 73  - Validation Accuracy: 0.6791999936103821\n",
            "Epoch 74  - Validation Accuracy: 0.6801999807357788\n",
            "Epoch 75  - Validation Accuracy: 0.6809999942779541\n",
            "Epoch 76  - Validation Accuracy: 0.6832000017166138\n",
            "Epoch 77  - Validation Accuracy: 0.6855999827384949\n",
            "Epoch 78  - Validation Accuracy: 0.6866000294685364\n",
            "Epoch 79  - Validation Accuracy: 0.6886000037193298\n",
            "Epoch 81  - Validation Accuracy: 0.6923999786376953\n",
            "Epoch 82  - Validation Accuracy: 0.692799985408783\n",
            "Epoch 83  - Validation Accuracy: 0.6952000260353088\n",
            "Epoch 84  - Validation Accuracy: 0.6967999935150146\n",
            "Epoch 85  - Validation Accuracy: 0.699400007724762\n",
            "Epoch 86  - Validation Accuracy: 0.7008000016212463\n",
            "Epoch 87  - Validation Accuracy: 0.7024000287055969\n",
            "Epoch 88  - Validation Accuracy: 0.704200029373169\n",
            "Epoch 89  - Validation Accuracy: 0.704800009727478\n",
            "Epoch 91  - Validation Accuracy: 0.7059999704360962\n",
            "Epoch 92  - Validation Accuracy: 0.7080000042915344\n",
            "Epoch 93  - Validation Accuracy: 0.7106000185012817\n",
            "Epoch 94  - Validation Accuracy: 0.7128000259399414\n",
            "Epoch 95  - Validation Accuracy: 0.7128000259399414\n",
            "Epoch 96  - Validation Accuracy: 0.7143999934196472\n",
            "Epoch 97  - Validation Accuracy: 0.7157999873161316\n",
            "Epoch 98  - Validation Accuracy: 0.7167999744415283\n",
            "Epoch 99  - Validation Accuracy: 0.7182000279426575\n",
            "Trained Model Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJUeXQbzFcfS",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12MTtoT8L42W",
        "colab_type": "code",
        "outputId": "48af192e-a618-43a5-e82b-1db39500b641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, save_file)\n",
        "    \n",
        "    test_accuracy = sess.run(\n",
        "        accuracy,\n",
        "        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\n",
        "    \n",
        "print('Test Accuracy: {}'.format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.7335000038146973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x57Uf0tYMi0G",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yW_QI6LMz1Q",
        "colab_type": "text"
      },
      "source": [
        "Sometimes you might want to adjust, or \"finetune\" a model that you have already trained and saved.\n",
        "\n",
        "However, loading saved Variables directly into a modified model can generate errors. Let's go over how to avoid these problems.\n",
        "\n",
        "#### Naming Error\n",
        "TensorFlow uses a string identifier for Tensors and Operations called name. If a name is not given, TensorFlow will create one automatically. TensorFlow will give the first node the name <Type>, and then give the name <Type>_<number> for the subsequent nodes. Let's see how this can affect loading a model with a different order of weights and bias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWwL5lZcM0vR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7aabd831-74db-40c4-b43a-76139b7a9afa"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Remove the previous weights and bias\n",
        "tf.reset_default_graph()\n",
        "\n",
        "save_file = 'model.ckpt'\n",
        "\n",
        "# Two Tensor Variables: weights and bias\n",
        "weights = tf.Variable(tf.truncated_normal([2,3]))\n",
        "bias = tf.Variable(tf.truncated_normal([3]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Print the name of Weights and Bias\n",
        "print(f'Save Weights: {weights.name}')\n",
        "print(f'Save Bias: {bias.name}')\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver.save(sess, save_file)\n",
        "    \n",
        "# Remove the previous weights and bias\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Two Variables: weights and bias\n",
        "bias = tf.Variable(tf.truncated_normal([3]))\n",
        "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Print the name of Weights and Bias\n",
        "print('Load Weights: {}'.format(weights.name))\n",
        "print('Load Bias: {}'.format(bias.name))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, save_file)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Weights: Variable:0\n",
            "Save Bias: Variable_1:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0715 14:15:44.530534 140298458589056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load Weights: Variable_1:0\n",
            "Load Bias: Variable:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[{{node save/Assign}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1286\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[node save/Assign (defined at <ipython-input-2-562695c868ca>:29) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/Assign:\n Variable (defined at <ipython-input-2-562695c868ca>:26)\n\nOriginal stack trace for 'save/Assign':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-562695c868ca>\", line 29, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 825, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 72, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-562695c868ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1320\u001b[0m       \u001b[0;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1322\u001b[0;31m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[node save/Assign (defined at <ipython-input-2-562695c868ca>:29) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/Assign:\n Variable (defined at <ipython-input-2-562695c868ca>:26)\n\nOriginal stack trace for 'save/Assign':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-562695c868ca>\", line 29, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 825, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 72, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlNnBFjI_30p",
        "colab_type": "text"
      },
      "source": [
        "…\n",
        "\n",
        "InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match.\n",
        "\n",
        "…\n",
        "\n",
        "You'll notice that the name properties for weights and bias are different than when you saved the model. This is why the code produces the \"Assign requires shapes of both tensors to match\" error. The code saver.restore(sess, save_file) is trying to load weight data into bias and bias data into weights.\n",
        "\n",
        "Instead of letting TensorFlow set the name property, let's set it manually:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDAkkBowM0lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d1e83a47-684c-4be2-921f-8b5d47cc6d07"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "save_file = 'model.ckpt'\n",
        "\n",
        "weights = tf.Variable(tf.truncated_normal([2,3]), name='weights_0')\n",
        "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "print('Save Weights: {}'.format(weights.name))\n",
        "print('Save Bias: {}'.format(bias.name))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver.save(sess, save_file)\n",
        "    \n",
        "tf.reset_default_graph()\n",
        "\n",
        "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
        "weights = tf.Variable(tf.truncated_normal([2,3]), name='weights_0')\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "print('Save Weights: {}'.format(weights.name))\n",
        "print('Save Bias: {}'.format(bias.name))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, save_file)\n",
        "\n",
        "print('Loaded Weights and Bias successfully.')   "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Weights: weights_0:0\n",
            "Save Bias: bias_0:0\n",
            "Save Weights: weights_0:0\n",
            "Save Bias: bias_0:0\n",
            "Loaded Weights and Bias successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1J60J1PMnI6",
        "colab_type": "text"
      },
      "source": [
        "# Quiz - Tensorflow Dropout\n",
        "\n",
        "- 正则化技巧防止过拟合\n",
        "- 临时drop一些神经元"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SpIt0oJM11C",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "# probability to keep units\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32) \n",
        "\n",
        "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
        "hidden_layer = tf.nn.relu(hidden_layer)\n",
        "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
        "\n",
        "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGlRmuLmM1u-",
        "colab_type": "text"
      },
      "source": [
        "The code above illustrates how to apply dropout to a neural network.\n",
        "\n",
        "The tf.nn.dropout() function takes in two parameters:\n",
        "\n",
        "hidden_layer: the tensor to which you would like to apply dropout\n",
        "keep_prob: the probability of keeping (i.e. not dropping) any given unit\n",
        "keep_prob allows you to adjust the number of units to drop. In order to compensate for dropped units, tf.nn.dropout() multiplies all units that are kept (i.e. not dropped) by 1/keep_prob.\n",
        "\n",
        "During training, a good starting value for keep_prob is 0.5.\n",
        "\n",
        "During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model.\n",
        "\n",
        "keep_prob should be set to 1.0 when evaluating validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrbiuwHyM1sn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cb648b2f-171b-40ea-bae7-f92f97da2a75"
      },
      "source": [
        "# Solution is available in the other \"solution.py\" tab\n",
        "import tensorflow as tf\n",
        "\n",
        "hidden_layer_weights = [\n",
        "    [0.1, 0.2, 0.4],\n",
        "    [0.4, 0.6, 0.6],\n",
        "    [0.5, 0.9, 0.1],\n",
        "    [0.8, 0.2, 0.8]]\n",
        "out_weights = [\n",
        "    [0.1, 0.6],\n",
        "    [0.2, 0.1],\n",
        "    [0.7, 0.9]]\n",
        "\n",
        "# Weights and biases\n",
        "weights = [\n",
        "    tf.Variable(hidden_layer_weights),\n",
        "    tf.Variable(out_weights)]\n",
        "biases = [\n",
        "    tf.Variable(tf.zeros(3)),\n",
        "    tf.Variable(tf.zeros(2))]\n",
        "\n",
        "# Input\n",
        "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
        "\n",
        "# TODO: Create Model with Dropout\n",
        "keep_prop = tf.placeholder(tf.float32)\n",
        "\n",
        "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
        "hidden_layer = tf.nn.relu(hidden_layer)\n",
        "hidden_layer = tf.nn.dropout(hidden_layer, keep_prop)\n",
        "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
        "# TODO: Print logits from a session\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    print(sess.run(logits, feed_dict={keep_prop:0.5}))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.9800003   7.5400004 ]\n",
            " [ 0.112       0.67200005]\n",
            " [ 4.7200003  28.320002  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBYyXtBmM1l6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}