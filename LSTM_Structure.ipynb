{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "LSTM Structure.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morbosohex/Workflow/blob/master/LSTM_Structure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34uh3qopGRMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "7a94178a-f747-4aea-acbe-cd16571d2c0c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD_uAy6fGJmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/AI算法工程师/LSTM Structure and Hidden State')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ves4JTgeGG_K",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Structure and Hidden State\n",
        "\n",
        "我们知道RNN用于通过将一个节点的输出链接到下一个节点的输入来维持一种存储器。 在LSTM的情况下，对于序列中的每个数据（例如，对于给定句子中的单词），存在对应的*隐藏状态* $ h_t $。 这种隐藏状态是LSTM随时间推移看到的数据的函数; 它包含一些权重，并代表LSTM已经看到的数据的短期和长期内存记忆。\n",
        "\n",
        "因此，对于正在查看句子中的单词的LSTM，**LSTM的隐藏状态将根据它看到的每个新单词而改变。 并且，我们可以使用隐藏状态来预测序列中的下一个单词**或帮助识别语言模型中的单词类型以及许多其他内容！\n",
        "\n",
        "### Exercise Repository\n",
        "\n",
        "Note that most exercise notebooks can be run locally on your computer, by following the directions in the [Github Exercise Repository](https://github.com/udacity/CVND_Exercises).\n",
        "\n",
        "\n",
        "## LSTMs in Pytorch\n",
        "\n",
        "创建和训练一个LSTM，你需要知道输入和隐藏态的结构，在PyTorch中LSTM可以通过语句`lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=n_layers)`.\n",
        "\n",
        "在PyTorch,LSTM希望得到的输入是一个3D张量，其维度定义如下：\n",
        ">* `input_dim` = the number of inputs (a dimension of 20 could represent 20 inputs,例如含有一定维度的单个词的独热向量，或某个维度的单个词的词向量，那么这些维度即为input_dim的维度)\n",
        ">* `hidden_dim` = 隐藏态的大小，在每个时间步，对于每一个LSTM单元里面的输出大小，例如一个单元中有四个神经元那么就是4\n",
        ">* `n_layers ` = LSTM层的数目，一般为1-3，例如1表示每一个LSTM单元对应一个隐藏态\n",
        "\n",
        "![image.png](https://upload-images.jianshu.io/upload_images/12735209-6b637c2989da1cb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
        "\n",
        "    \n",
        "### Hidden State\n",
        "一旦使用输入和隐藏维度定义了LSTM，我们就可以调用它并在每个时间步检索输出和隐藏状态。  \n",
        "\n",
        "`out，hidden = lstm（input.view（1,1，-1），（h0，c0))`\n",
        "\n",
        "\n",
        "LSTM的输入是**（input，（h0，c0））`**。\n",
        "\n",
        ">* `input` = 一个包含输入序列值的张量，该值的组成（seq_len，batch，input_size）\n",
        "第一个轴是序列本身,例如序列有5个词，那么就是5，第二个轴是迷你批次中的索引实例，例如批次可以设定为64，为了方便理解可以设为1，第三个索引是输入的元素，例如对于序列中的每一个词通常有两种表示，一种是独热向量，那么长度就是独热向量的维度，另一种是词向量表示，那么就是词向量的维度。我们还没有讨论过迷你批处理，所以让我们忽略它并假设我们在第二轴上总是只有1维。下图演示了批次为1的input，对于下图序列长度为3， batch为1，input_size = len(q_单词）\n",
        "![image.png](https://upload-images.jianshu.io/upload_images/12735209-45ef957ce7750b8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
        "\n",
        "\n",
        "\n",
        ">* `h0` = 每个批次中每个元素的初始化隐藏态的张量\n",
        ">* `c0` = 在批次中的每个元素的初始化LSTM单元记忆的张量\n",
        "\n",
        "`h0` 和 `c0`默认为0，如果没有指定的话。它们的尺寸为：（n_layers，batch，hidden_​​dim）。\n",
        "\n",
        "详情见该文件[this PyTorch LSTM tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#lstm-s-in-pytorch).\n",
        "\n",
        "让我们举一个简单的例子说我们想通过LSTM处理一个句子。如果我们想在一个句子\"Giraffes in a field\"中运行序列模型，我们的输入应该看起来像这个单个单词的'1x4`行向量：\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "   \\text{Giraffes  } \n",
        "   \\text{in  } \n",
        "   \\text{a  } \n",
        "   \\text{field} \n",
        "   \\end{bmatrix}\\end{align}\n",
        "\n",
        "在这种情况下，我们知道我们有** 4个输入字**，我们决定在每个时间步产生多少输出，比方说我们希望每个LSTM单元产生** 3个隐藏状态值**。我们将LSTM中的图层数保持为默认大小1。\n",
        "\n",
        "隐藏状态和LSTM单元格内存将具有维度（n_layers，batch，hidden_​​dim），在这种情况下，对于1层模型，它将是（1,1,3），其中一个批处理/单词序列要处理（这一个句子）和3个经过处理的隐藏状态值。\n",
        "\n",
        "\n",
        "### Example Code\n",
        "\n",
        "接下来，让我们看一个LSTM的示例，该LSTM旨在查看4个值的序列（数值，因为这些值最容易创建和跟踪）并生成3个值作为输出。这就是上面的句子处理网络的样子，并鼓励你改变这些输入/隐藏状态大小，看看对LSTM结构的影响！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W28yCIkFGG_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f44b2fb9-0dfc-482a-de62-ed9533af576e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "torch.manual_seed(2) # so that random variables will be consistent and repeatable for testing"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f69793e5a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjqDqUBeGG_P",
        "colab_type": "text"
      },
      "source": [
        "### 定义一个简单的LSTM\n",
        "\n",
        "\n",
        "**输出层和隐藏层的维度**\n",
        "\n",
        "`hidden_dim` 和 size of the output 一般是相同的，除非定义一个特定的LSTM，改变输出单元的数量，通过在网络末端增加一个线性层 例如 `fc = nn.Linear(hidden_dim, output_dim)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLi_dli8GG_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "523859ab-3c43-437f-be8f-57da24d68330"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "# define an LSTM with an input dim of 4 and hidden dim of 3\n",
        "# this expects to see 4 values as input and generates 3 values as output\n",
        "input_dim = 4\n",
        "hidden_dim = 3\n",
        "lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim) \n",
        "# make 5 input sequences of 4 random values each\n",
        "inputs_list = [torch.randn(1, input_dim) for _ in range(5)]\n",
        "print('inputs: \\n', inputs_list)\n",
        "print('\\n')\n",
        "\n",
        "# initialize the hidden state\n",
        "# (n_layers, batch_size, hidden_dim)\n",
        "# (1 layer, 1 batch_size, 3 outputs)\n",
        "# first tensor is the hidden state, h0\n",
        "# second tensor initializes the cell memory, c0\n",
        "h0 = torch.randn(1, 1, hidden_dim)\n",
        "c0 = torch.randn(1, 1, hidden_dim)\n",
        "\n",
        "\n",
        "h0 = Variable(h0)\n",
        "c0 = Variable(c0)\n",
        "#print('h0',h0)\n",
        "#print('c0',c0)\n",
        "# step through the sequence one element at a time.\n",
        "for i in inputs_list:\n",
        "    # wrap in Variable \n",
        "    i = Variable(i)\n",
        "    \n",
        "    # after each step, hidden contains the hidden state\n",
        "    out, hidden = lstm(i.view(1, 1, -1), (h0, c0))\n",
        "    print('out: \\n', out)\n",
        "    print('hidden: \\n', hidden)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs: \n",
            " [tensor([[1.4934, 0.4987, 0.2319, 1.1746]]), tensor([[-1.3967,  0.8998,  1.0956, -0.5231]]), tensor([[-0.8462, -0.9946,  0.6311,  0.5327]]), tensor([[-0.8454,  0.9406, -2.1224,  0.0233]]), tensor([[ 0.4836,  1.2895,  0.8957, -0.2465]])]\n",
            "\n",
            "\n",
            "out: \n",
            " tensor([[[-0.4372,  0.2583,  0.2947]]], grad_fn=<StackBackward>)\n",
            "hidden: \n",
            " (tensor([[[-0.4372,  0.2583,  0.2947]]], grad_fn=<StackBackward>), tensor([[[-0.7344,  0.6209,  0.4191]]], grad_fn=<StackBackward>))\n",
            "out: \n",
            " tensor([[[-0.2836,  0.1314,  0.4133]]], grad_fn=<StackBackward>)\n",
            "hidden: \n",
            " (tensor([[[-0.2836,  0.1314,  0.4133]]], grad_fn=<StackBackward>), tensor([[[-0.5041,  0.2672,  0.6370]]], grad_fn=<StackBackward>))\n",
            "out: \n",
            " tensor([[[-0.3404,  0.4880,  0.1949]]], grad_fn=<StackBackward>)\n",
            "hidden: \n",
            " (tensor([[[-0.3404,  0.4880,  0.1949]]], grad_fn=<StackBackward>), tensor([[[-0.5552,  0.7909,  0.3300]]], grad_fn=<StackBackward>))\n",
            "out: \n",
            " tensor([[[-0.3544,  0.2405,  0.3150]]], grad_fn=<StackBackward>)\n",
            "hidden: \n",
            " (tensor([[[-0.3544,  0.2405,  0.3150]]], grad_fn=<StackBackward>), tensor([[[-0.5645,  1.0073,  0.6101]]], grad_fn=<StackBackward>))\n",
            "out: \n",
            " tensor([[[-0.3328,  0.0437,  0.3817]]], grad_fn=<StackBackward>)\n",
            "hidden: \n",
            " (tensor([[[-0.3328,  0.0437,  0.3817]]], grad_fn=<StackBackward>), tensor([[[-0.5311,  0.1181,  0.5304]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Q8L2cGGG_S",
        "colab_type": "text"
      },
      "source": [
        "You should see that the output and hidden Tensors are always of length 3, which we specified when we defined the LSTM with `hidden_dim`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdzRyeGbGG_T",
        "colab_type": "text"
      },
      "source": [
        "### All at once\n",
        "对于长序列数据，简单的for循环并不是很高效，因此可以将这些inputs一次性处理\n",
        "\n",
        "1. concat 所有的input序列为一个大的张量，将其定义为一个batch_size\n",
        "2. 定义输出隐藏态的shape\n",
        "3. 获取输出和由上一个词产生的隐藏态\n",
        "\n",
        "输出可能看起来有些些许不同，因为隐藏态初始化的不同"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULxWdLbCGG_U",
        "colab_type": "code",
        "colab": {},
        "outputId": "0fe3f59d-64a2-4146-a103-7cc6f1a1b3d0"
      },
      "source": [
        "# turn inputs into a tensor with 5 rows of data\n",
        "# add the extra 2nd dimension (1) for batch_size\n",
        "inputs = torch.cat(inputs_list).view(len(inputs_list), 1, -1)\n",
        "\n",
        "# print out our inputs and their shape\n",
        "# you should see (number of sequences, batch size, input_dim)\n",
        "print('inputs size: \\n', inputs.size())\n",
        "print('\\n')\n",
        "\n",
        "print('inputs: \\n', inputs)\n",
        "print('\\n')\n",
        "\n",
        "# initialize the hidden state\n",
        "h0 = torch.randn(1, 1, hidden_dim)\n",
        "c0 = torch.randn(1, 1, hidden_dim)\n",
        "\n",
        "# wrap everything in Variable\n",
        "inputs = Variable(inputs)\n",
        "h0 = Variable(h0)\n",
        "c0 = Variable(c0)\n",
        "# get the outputs and hidden state\n",
        "out, hidden = lstm(inputs, (h0, c0))\n",
        "\n",
        "print('out: \\n', out)\n",
        "print('hidden: \\n', hidden)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs size: \n",
            " torch.Size([5, 1, 4])\n",
            "\n",
            "\n",
            "inputs: \n",
            " tensor([[[ 1.4934,  0.4987,  0.2319,  1.1746]],\n",
            "\n",
            "        [[-1.3967,  0.8998,  1.0956, -0.5231]],\n",
            "\n",
            "        [[-0.8462, -0.9946,  0.6311,  0.5327]],\n",
            "\n",
            "        [[-0.8454,  0.9406, -2.1224,  0.0233]],\n",
            "\n",
            "        [[ 0.4836,  1.2895,  0.8957, -0.2465]]])\n",
            "\n",
            "\n",
            "out: \n",
            " tensor([[[ 0.1611,  0.2200,  0.2213]],\n",
            "\n",
            "        [[ 0.0364, -0.0390,  0.2638]],\n",
            "\n",
            "        [[-0.1425, -0.0174,  0.1504]],\n",
            "\n",
            "        [[-0.1583,  0.1264,  0.1709]],\n",
            "\n",
            "        [[-0.2007, -0.1559,  0.2489]]])\n",
            "hidden: \n",
            " (tensor([[[-0.2007, -0.1559,  0.2489]]]), tensor([[[-0.4429, -0.2975,  0.3252]]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Dd7BMQGG_W",
        "colab_type": "text"
      },
      "source": [
        "### Next: 隐藏态和门\n",
        "该notebook文件主要展示了input和output的结构在pytorch中是怎样的。接下来你将学习到更多的关于长期记忆和短期记忆有关隐藏态的。\n",
        "\n",
        "#### 词性标注\n",
        "之后的notebook中，将会定一个词性标注模型（其中包含三种词性，分别是名词，动词，冠词），以及定义一个目标输出size的线性层， 并且最后训练该模型生成一个概率分布有关这三种种类的"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkrEH5m3GG_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}