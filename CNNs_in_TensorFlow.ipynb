{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNs in TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morbosohex/Workflow/blob/master/CNNs_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAExl72zJsx1",
        "colab_type": "text"
      },
      "source": [
        "# 1. Convolutional Layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS6q_LGTOUQ2",
        "colab_type": "text"
      },
      "source": [
        "### 卷积层\n",
        "\n",
        "下图是使用3x3过滤器和步长为1进行卷积的示例。\n",
        "\n",
        "![image.png](https://upload-images.jianshu.io/upload_images/12735209-6903c153ff580bfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
        "\n",
        "根据权重计算每个3x3部分的卷积[[1, 0, 1], [0, 1, 0], [1, 0, 1]]，然后加上偏差以在右侧创建卷积特征。在这种情况下，偏差为零。\n",
        "\n",
        "\n",
        "### TensorFlow中的卷积层\n",
        "让我们来看看如何在TensorFlow中实现卷积层。\n",
        "\n",
        "TensorFlow提供了tf.nn.conv2d()，tf.nn.bias_add()和tf.nn.relu()函数来创建自己的卷积层。\n",
        "```python\n",
        "# output depth\n",
        "k_output = 64\n",
        "\n",
        "# image dimensions\n",
        "image_width = 10\n",
        "image_height = 10\n",
        "color_channels = 3\n",
        "\n",
        "# convolution filter dimensions\n",
        "filter_size_width = 5\n",
        "filter_size_height = 5\n",
        "\n",
        "# input/image\n",
        "input = tf.placeholder(\n",
        "    tf.float32,\n",
        "    shape=[None, image_height, image_width, color_channels])\n",
        "\n",
        "# weight and bias\n",
        "weight = tf.Variable(tf.truncated_normal(\n",
        "    [filter_size_height, filter_size_width, color_channels, k_output]))\n",
        "bias = tf.Variable(tf.zeros(k_output))\n",
        "\n",
        "# apply convolution\n",
        "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
        "# add bias\n",
        "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
        "# apply activation function\n",
        "conv_layer = tf.nn.relu(conv_layer)\n",
        "```\n",
        "\n",
        "上面的代码使用tf.nn.conv2d()函数计算卷积weight作为过滤器和`[1, 2, 2, 1]`步幅。\n",
        "\n",
        "- TensorFlow为每个input维度使用一个步长, 如`[batch, input_height, input_width, input_channels]`。\n",
        "- 我们通常总是设置`batch`和`input_channels`的步长为（即`strides`数组中的第一个和第四个元素）1。这可确保模型所有批次和输入通道都被使用。（如果某些批次和通道不使用, 最好从数据集中删除要跳过的批次或通道，而不是使用步长跳过它们。）\n",
        "- 只需要设置`input_height`和 `input_width` 的步长通常（在设置`batch`和`input_channels`为1）。此示例代码使用步长为2和`5x5`的过滤器input。通常有一个方形的步幅`height = width`。当有人说他们正在使用2的步幅时，他们通常意味着`tf.nn.conv2d(x, W, strides=[1, 2, 2, 1])`。\n",
        "\n",
        "该`tf.nn.bias_add()`函数在矩阵的最后一维上增加了一维偏差。（注意：`tf.add()`当张量不同的形状时，使用不起作用。）\n",
        "\n",
        "该`tf.nn.relu()`功能将ReLU激活功能应用于图层。\n",
        "\n",
        "### Quiz: 在tensorflow中构建卷积层\n",
        "\n",
        "现在让我们在TensorFlow中构建一个卷积层。在下面的练习中，将要求您设置卷积过滤器的shape，权重和偏差。这在很多方面是在TensorFlow中使用CNN最棘手的部分。一旦您了解了如何设置这些属性的维度，应用CNN将更加直截了当。\n",
        "\n",
        "Instructions\n",
        "- Finish off each TODO in the conv2d function.\n",
        "- Set up the strides, padding, filter weight (F_w), and filter bias (F_b)\n",
        "- the output shape is (1, 2, 2, 3). Note that all of these except strides should be TensorFlow variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhy5fDgsUedU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Setup the strides, padding and filter weight/bias such that\n",
        "the output shape is (1, 2, 2, 3).\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n",
        "# (1, 4, 4, 1)\n",
        "x = np.array([\n",
        "    [0, 1, 0.5, 10],\n",
        "    [2, 2.5, 1, -8],\n",
        "    [4, 0, 5, 6],\n",
        "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
        "X = tf.constant(x)\n",
        "\n",
        "\n",
        "def conv2d(input):\n",
        "    # Filter (weights and bias)\n",
        "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
        "    # The shape of the filter bias is (output_depth,)\n",
        "    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n",
        "    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n",
        "    F_W = tf.Variable(tf.truncated_normal([2,\n",
        "                                           2,\n",
        "                                           1,\n",
        "                                           3]))\n",
        "    F_b = tf.Variable(tf.zeros(3))\n",
        "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
        "    strides = [1, 2, 2, 1]\n",
        "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
        "    padding = 'SAME'\n",
        "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n",
        "    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n",
        "    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\n",
        "\n",
        "out = conv2d(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYuivDkCV94k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6219986f-c5a5-4d3f-b390-dc72bd86c2aa"
      },
      "source": [
        "print(out)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"add:0\", shape=(1, 2, 2, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVYaoZ-9J_8x",
        "colab_type": "text"
      },
      "source": [
        "# 2. Max Pooling Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL6n6otDXvAL",
        "colab_type": "text"
      },
      "source": [
        "### TensorFlow中的最大池化层\n",
        "\n",
        "下图是使用2x2过滤器和步幅为2 的最大池化的示例。四个2x2颜色区域表示每次应用过滤器以查找每个区域的最大值。\n",
        "\n",
        "![image.png](https://upload-images.jianshu.io/upload_images/12735209-56d8be13c357beb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
        "\n",
        "例如，`[[1, 0], [4, 6]]`变为`6`，因为`6`是此集合中的最大值。同样，`[[2, 3], [6, 8]]`成为`8`。\n",
        "\n",
        "从概念上讲，最大池化操作的好处是减小输入的大小，并允许神经网络只关注最重要的元素。最大池化仅通过保留每个过滤区域的最大值并删除其余值来实现此目的。\n",
        "\n",
        "TensorFlow提供了 `tf.nn.max_pool()`将最大池化应用于卷积层的函数。\n",
        "\n",
        "```python\n",
        "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
        "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
        "conv_layer = tf.nn.relu(conv_layer)\n",
        "# apply max pooling\n",
        "conv_layer = tf.nn.max_pool(\n",
        "    conv_layer,\n",
        "    ksize=[1, 2, 2, 1],\n",
        "    strides=[1, 2, 2, 1],\n",
        "    padding='SAME')\n",
        "```\n",
        "\n",
        "该`tf.nn.max_pool()`函数执行最大池化，`ksize`参数作为过滤器的大小，`strides`参数作为步幅的长度。在实践中，步长为2x2的滤波器很常见。\n",
        "\n",
        "`ksize`和`strides`参数被构造为4元素列表，其中相应于输入张量的尺寸的每个元素（`[batch, height, width, channels]`）。对于这两个`ksize`和`strides`，batch和channel shape通常设定为1。\n",
        "\n",
        "### Quiz - 在TensorFlow中使用Max Pooling图层\n",
        "\n",
        "Instructions\n",
        "- Finish off each TODO in the maxpool function.\n",
        "\n",
        "- Setup the strides, padding and ksize such that the output shape after pooling is (1, 2, 2, 1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgo4djkEJfTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45594295-4021-4685-ef84-d6df3c6adbce"
      },
      "source": [
        "\"\"\"\n",
        "Set the values to `strides` and `ksize` such that\n",
        "the output shape after pooling is (1, 2, 2, 1).\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\n",
        "# (1, 4, 4, 1)\n",
        "x = np.array([\n",
        "    [0, 1, 0.5, 10],\n",
        "    [2, 2.5, 1, -8],\n",
        "    [4, 0, 5, 6],\n",
        "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
        "X = tf.constant(x)\n",
        "\n",
        "def maxpool(input):\n",
        "    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n",
        "    ksize = [1, 2, 2, 1]\n",
        "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
        "    strides = [1, 2, 2, 1]\n",
        "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
        "    padding = \"SAME\"\n",
        "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\n",
        "    return tf.nn.max_pool(input, ksize, strides, padding)\n",
        "    \n",
        "out = maxpool(X)\n",
        "print(out)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"MaxPool:0\", shape=(1, 2, 2, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKF3xio2KSTD",
        "colab_type": "text"
      },
      "source": [
        "# 3. CNN in TensorFlow\n",
        "\n",
        "![image.png](https://upload-images.jianshu.io/upload_images/12735209-6fec1b6786f533a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
        "\n",
        "该网络的结构遵循CNN的经典结构，CNN是卷积层和最大池的混合，其次是完全连接的层。\n",
        "\n",
        "### 数据集\n",
        "在这里，我们导入MNIST数据集并使用方便的TensorFlow函数对数据进行批处理，缩放和单热编码。\n",
        "\n",
        "```python\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.00001\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "# number of samples to calculate validation and accuracy\n",
        "# decrease this if you're running out of memory\n",
        "test_valid_size = 256\n",
        "\n",
        "# network Parameters\n",
        "n_classes = 10  # MNIST total classes (0-9 digits)\n",
        "dropout = 0.75  # dropout (probability to keep units)\n",
        "```\n",
        "\n",
        "### 权重和偏差\n",
        "在下面的代码中，我们将创建3个层，在卷积和最大池之间交替，然后是完全连接和输出层。我们首先定义必要的权重和偏差。\n",
        "\n",
        "```python\n",
        "# store weights & biases\n",
        "weights = {\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
        "```\n",
        "\n",
        "### 卷积层\n",
        "回想一下，TensorFlow提供了`tf.nn.conv2d()`，`tf.nn.bias_add()`和`tf.nn.relu()`函数来创建自己的卷积层。为了简化代码，我们在下面定义了一个有用的函数。\n",
        "\n",
        "```python\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    x = tf.nn.relu(x)\n",
        "    return x\n",
        "```\n",
        "\n",
        "该conv2d函数在应用ReLU激活函数之前计算卷积与权重W，然后增加偏差b。\n",
        "\n",
        "### 最大池层\n",
        "\n",
        "TensorFlow提供了 `tf.nn.max_pool()`将最大池应用于卷积层的功能。为了简化代码，我们在下面定义了一个有用的函数。\n",
        "\n",
        "```python\n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(\n",
        "        x,\n",
        "        ksize=[1,k,k,1],\n",
        "        strides=[1,k,k,1],\n",
        "        padding=\"SAME\")\n",
        "```\n",
        "\n",
        "该`maxpool2d`函数`x`使用大小的过滤器将最大池应用于图层k。\n",
        "\n",
        "### 模型\n",
        "每个层到新维度的转换显示在注释中。例如，第一层在卷积步骤中将图像从28x28x1到28x28x32整形。下一步应用最大池，将每个样本转换为14x14x32。所有的层是由施加conv1到output，产生10类预测。\n",
        "\n",
        "#### tips:\n",
        "```\n",
        "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
        "out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
        "```\n",
        "个层到新维度的转换显示在注释中。例如，第一层在卷积步骤中将图像从28x28x1到28x28x32整形。下一步应用最大池，将每个样本转换为14x14x32。所有的层是由施加conv1到output，产生10类预测。\n",
        "\n",
        "```python\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    # Layer 1 - 28*28*1 to 14*14*32\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Layer 2 - 14*14*32 to 7*7*64\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer - 7*7*64 to 1024\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output Layer - class prediction - 1024 to 10\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out\n",
        "```\n",
        "\n",
        "### 会话\n",
        "```python\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# Model\n",
        "logits = conv_net(x, weights, biases, keep_prob)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(\\\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
        "    .minimize(cost)\n",
        "\n",
        "# Accuracy\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf. global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(mnist.train.num_examples//batch_size):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            sess.run(optimizer, feed_dict={\n",
        "                x: batch_x,\n",
        "                y: batch_y,\n",
        "                keep_prob: dropout})\n",
        "\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss = sess.run(cost, feed_dict={\n",
        "                x: batch_x,\n",
        "                y: batch_y,\n",
        "                keep_prob: 1.})\n",
        "            valid_acc = sess.run(accuracy, feed_dict={\n",
        "                x: mnist.validation.images[:test_valid_size],\n",
        "                y: mnist.validation.labels[:test_valid_size],\n",
        "                keep_prob: 1.})\n",
        "\n",
        "            print('Epoch {:>2}, Batch {:>3} -'\n",
        "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
        "                epoch + 1,\n",
        "                batch + 1,\n",
        "                loss,\n",
        "                valid_acc))\n",
        "\n",
        "    # Calculate Test Accuracy\n",
        "    test_acc = sess.run(accuracy, feed_dict={\n",
        "        x: mnist.test.images[:test_valid_size],\n",
        "        y: mnist.test.labels[:test_valid_size],\n",
        "        keep_prob: 1.})\n",
        "    print('Testing Accuracy: {}'.format(test_acc))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2qO0mDWSdPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}